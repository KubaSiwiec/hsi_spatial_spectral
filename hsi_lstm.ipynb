{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hsi_lstm.ipynb",
      "provenance": [],
      "mount_file_id": "13WsY2YGPifeX-7Rv7P0wgbOcgFDm6Prt",
      "authorship_tag": "ABX9TyOPQMZyWOHLMO1A+/V4Rq5s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KubaSiwiec/hsi_spatial_spectral/blob/collab/hsi_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sXnDmXR7RDr2",
        "outputId": "5bf5dea7-9eb8-4d4c-b0c7-525d1851ac8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dWXpdkY4FNz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "feature extraction functions\n",
        "'''\n",
        "\n",
        "import scipy.io\n",
        "from skimage.util import view_as_windows\n",
        "from skimage.util import pad\n",
        "import numpy as np\n",
        "from skimage.feature import local_binary_pattern, hog\n",
        "\n",
        "\n",
        "def get_data(file_name):\n",
        "    return scipy.io.loadmat(file_name)\n",
        "\n",
        "def get_patches(image, size = 3, use_padding = 'SAME'):\n",
        "\n",
        "    # add padding for image to make it able to extract the number of patches equal to number of pixels\n",
        "    # the pad width should eqal (patch size - 1) / 2,  - center pixel will always lay in original image\n",
        "    nb_padding_pixels = int(size/2 - 0.5)\n",
        "    # choose symmetric mode\n",
        "    image_padded = pad(image, nb_padding_pixels, 'symmetric')\n",
        "\n",
        "    patch_size = (size, size, image.shape[2])\n",
        "    patches = view_as_windows(image_padded, patch_size)[:, :, 0]\n",
        "    return patches\n",
        "\n",
        "def hsi_to_lbp(image, radius, n_points):\n",
        "\n",
        "    image_shape = image.shape\n",
        "    image_depth = image_shape[2]\n",
        "\n",
        "\n",
        "    lbp_map = np.zeros(image.shape)\n",
        "    \n",
        "    for k in range(image_depth):\n",
        "        # compare each pixel greyscale value with the central pixel\n",
        "        lbp_map[:, :, k] = local_binary_pattern(image[:, :, k], n_points, radius)\n",
        "\n",
        "    return lbp_map\n",
        "\n",
        "\n",
        "def getCLBPlayer(image, radius):\n",
        "    '''\n",
        "    == Input ==\n",
        "    gray_image  : color image of shape (height, width)\n",
        "    \n",
        "    == Output ==  \n",
        "    imgLBP : LBP converted image of the same shape as \n",
        "    '''\n",
        "    \n",
        "\n",
        "    imgCLBP = np.zeros(image.shape)\n",
        "    radius = 3 \n",
        "    for ih in range(0,image.shape[0] - radius):\n",
        "        for iw in range(0,image.shape[1] - radius):\n",
        "            ### Step 1: 3 by 3 pixel\n",
        "            img = image[ih:ih+radius,iw:iw+radius]\n",
        "            img_mean = np.mean(img)\n",
        "            img01 = (img >= img_mean)*1.0\n",
        "            img01_vector = img01.T.flatten()\n",
        "            # it is ok to order counterclock manner\n",
        "            # img01_vector = img01.flatten()\n",
        "            ### Step 2: **Binary operation**:\n",
        "            img01_vector = np.delete(img01_vector,4)\n",
        "            ### Step 3: Decimal: Convert the binary operated values to a digit.\n",
        "            where_img01_vector = np.where(img01_vector)[0]\n",
        "            if len(where_img01_vector) >= 1:\n",
        "                num = np.sum(2**where_img01_vector)\n",
        "            else:\n",
        "                num = 0\n",
        "            imgCLBP[ih+1,iw+1] = num\n",
        "    return(imgCLBP)\n",
        "\n",
        "def hsi_to_clbp(image, radius):\n",
        "\n",
        "    image_shape = image.shape\n",
        "    image_depth = image_shape[2]\n",
        "\n",
        "\n",
        "    clbp_map = np.zeros(image.shape)\n",
        "    \n",
        "    for k in range(image_depth):\n",
        "        # compare each pixel greyscale value with the central pixel\n",
        "        clbp_map[:, :, k] = getCLBPlayer(image[:, :, k], radius)\n",
        "\n",
        "    return clbp_map\n",
        "\n",
        "\n",
        "def hsi_to_clbp(image, radius):\n",
        "\n",
        "    image_shape = image.shape\n",
        "    image_depth = image_shape[2]\n",
        "\n",
        "    hog_lst = []\n",
        "    \n",
        "    for k in range(image_depth):\n",
        "        # compare each pixel greyscale value with the central pixel\n",
        "        hog_lst.append(hog(image[:, :, k]))\n",
        "\n",
        "    hog_arr = np.asarray(hog_lst)\n",
        "\n",
        "    return hog_arr\n",
        "\n",
        "\n",
        "def hsi_to_wtf(image):\n",
        "\n",
        "    image_shape = image.shape\n",
        "    image_width = image_shape[0]\n",
        "    image_height = image_shape[1]\n",
        "    image_depth = image_shape[2]\n",
        "\n",
        "    # print(\"Shape: {}, Width: {}, Height: {}, Depth: {}\".format(image_shape, image_width, image_height, image_depth))\n",
        "\n",
        "    #get the image shape\n",
        "    central_pixel = image[int((image_width - 1) / 2), int((image_height - 1) / 2), :]\n",
        "\n",
        "    # print(\"Central pixel: {}\".format(central_pixel))\n",
        "\n",
        "    lbp_map = np.zeros(image.shape)\n",
        "    # dimensions of the image: (image_width,image_height,image_depth)\n",
        "    # for each of k channels\n",
        "    for k in range(image_depth):\n",
        "        # compare each pixel greyscale value with the central pixel\n",
        "        for i in range(image_width):\n",
        "            for j in range(image_height):\n",
        "                if image[i,j,k] >= central_pixel[k]:\n",
        "                    lbp_map[i,j,k] = True\n",
        "                elif image[i,j,k] < central_pixel[k]:\n",
        "                    lbp_map[i,j,k] = False\n",
        "                # setting the central pixel to 0.5 removes the noisy pixel from the middle of a image\n",
        "                #but LBP map stops to be binary\n",
        "                #when using it, remove the eqaul sign from the first conditional equation\n",
        "                # else:\n",
        "                #     lbp_map[i,j,k] = 0.5\n",
        "\n",
        "    print('lbp map: {}'.format(lbp_map))\n",
        "\n",
        "    return lbp_map\n",
        "\n",
        "\n",
        "def hsi_to_mwtf(image):\n",
        "    #here, the treshold will be set as mean of the image, not as value of specific central pixel\n",
        "\n",
        "    image_shape = image.shape\n",
        "    image_width = image_shape[0]\n",
        "    image_height = image_shape[1]\n",
        "    image_depth = image_shape[2]\n",
        "\n",
        "    # print(\"Shape: {}, Width: {}, Height: {}, Depth: {}\".format(image_shape, image_width, image_height, image_depth))\n",
        "\n",
        "    #get the image shape\n",
        "    central_pixel = image[int((image_width - 1) / 2), int((image_height - 1) / 2), :]\n",
        "\n",
        "    # print(\"Central pixel: {}\".format(central_pixel))\n",
        "\n",
        "    clbp_map = np.zeros(image.shape)\n",
        "    # dimensions of the image: (image_width,image_height,image_depth)\n",
        "    # for each of k channels\n",
        "    for k in range(image_depth):\n",
        "        # compare each pixel greyscale value with the mean of the image\n",
        "\n",
        "        #get mean of a layer\n",
        "        channel_mean = np.mean(image[:, :, k])\n",
        "        for i in range(image_width):\n",
        "            for j in range(image_height):\n",
        "                if image[i,j,k] >= channel_mean:\n",
        "                    clbp_map[i,j,k] = 1\n",
        "                elif image[i,j,k] < channel_mean:\n",
        "                    clbp_map[i,j,k] = 0\n",
        "\n",
        "    return clbp_map\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def hs_to_grey(image):\n",
        "    return np.mean(image, axis = 2)\n",
        "\n",
        "def arr2D_to_list(arr: np.array):\n",
        "    dims = len(arr.shape)\n",
        "    lst = []\n",
        "    if dims == 2:\n",
        "        print('Gt width: {}, length: {}'.format(arr.shape[0], arr.shape[1]))\n",
        "        for i in range(arr.shape[0]):\n",
        "            for j in range(arr.shape[1]):\n",
        "                lst.append(arr[i, j])\n",
        "\n",
        "        return lst\n",
        "    else:\n",
        "        raise Exception('Array should be two dimentional')\n",
        "\n",
        "def arr5D_to_list_of_3D_arr(arr: np.array):\n",
        "    dims = len(arr.shape)\n",
        "    lst = []\n",
        "    if dims == 5:\n",
        "        for i in range(arr.shape[0]):\n",
        "            for j in range(arr.shape[1]):\n",
        "                    lst.append(arr[i, j])\n",
        "        return lst\n",
        "    else:\n",
        "        raise Exception('Array should be two dimentional')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYjV6R59l55d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gc\n",
        "class GarbageCollectorCallback(tf.keras.callbacks.Callback):\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    gc.collect()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9yw3o594TxW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from skimage.transform import resize\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZrPJE13RRXU",
        "colab_type": "code",
        "outputId": "c3229a2e-f752-4615-a33c-78fbec56bcca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "mat = get_data(\"/content/drive/My Drive/PaviaU.mat\")\n",
        "# print(mat)\n",
        "\n",
        "mat_gt = get_data(\"/content/drive/My Drive/PaviaU_gt.mat\")\n",
        "# print(mat_gt\n",
        "\n",
        "data = mat['paviaU']\n",
        "ground_truth = mat_gt['paviaU_gt']\n",
        "\n",
        "print(\"Shape of the cube: {}\".format(data.shape))\n",
        "print(\"Shape of the labels_arr: {}\".format(ground_truth.shape))\n",
        "print(\"Number of classes: {}\".format(np.unique(ground_truth)))\n",
        "\n",
        "print(\"Maximum value: {}\".format(np.argmax(data)))\n",
        "\n",
        "print(\"Index of maximum value: {}\".format(np.unravel_index(np.argmax(data), data.shape)))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of the cube: (610, 340, 103)\n",
            "Shape of the labels_arr: (610, 340)\n",
            "Number of classes: [0 1 2 3 4 5 6 7 8 9]\n",
            "Maximum value: 396299\n",
            "Index of maximum value: (11, 107, 58)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNe_VQtPRR2U",
        "colab_type": "code",
        "outputId": "154fb62d-7b00-4404-8d7b-e826d7fd6df8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 903
        }
      },
      "source": [
        "# apply lbp and clbp on image\n",
        "radius = 3\n",
        "processed_image = hsi_to_lbp(data, radius, radius*8)\n",
        "print('LBP image shape: {}'.format(processed_image.shape))\n",
        "print(np.int_(processed_image/10000))\n",
        "\n",
        "processed_image = np.int_(processed_image/10000/1677*255)\n",
        "print(np.max(processed_image))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LBP image shape: (610, 340, 103)\n",
            "[[[1546 1546 1336 ...    0    0    0]\n",
            "  [1612 1585  249 ...    0    0    0]\n",
            "  [1425    3    9 ...    0    0    0]\n",
            "  ...\n",
            "  [ 418  179    1 ...    3    3    4]\n",
            "  [ 101  104  209 ...   84   84   84]\n",
            "  [  49   49   50 ...   52   52   52]]\n",
            "\n",
            " [[1284   26   26 ...    0    0    0]\n",
            "  [   0   13  249 ...    0    0    0]\n",
            "  [ 835 1582    9 ...    0    0    0]\n",
            "  ...\n",
            "  [   0  186  396 ...    0    0    0]\n",
            "  [  13    0    0 ...   81   81   81]\n",
            "  [   0    0   49 ...   52   52   52]]\n",
            "\n",
            " [[   0    0   26 ...    0    0    0]\n",
            "  [1671 1297   13 ...   39   39   39]\n",
            "  [   0  629 1622 ... 1671 1671 1671]\n",
            "  ...\n",
            "  [ 210  106  393 ...  209  209  209]\n",
            "  [ 209  205   52 ...  101  101  101]\n",
            "  [   3   49   51 ...   46   46   46]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[1468 1468 1468 ...  838 1258 1258]\n",
            "  [1468  838    0 ... 1258 1258 1468]\n",
            "  [1468 1258    0 ... 1468 1468 1468]\n",
            "  ...\n",
            "  [   0    0    0 ...    0    0    1]\n",
            "  [   6    6 1677 ...    0    0    0]\n",
            "  [   0    0    0 ...    1    1    1]]\n",
            "\n",
            " [[ 838  838    0 ...  838  838  838]\n",
            "  [ 838    0  838 ...  838  838  838]\n",
            "  [   0    0    0 ...  838  838  838]\n",
            "  ...\n",
            "  [   0    0    0 ...    0    0    0]\n",
            "  [   1    1    1 ...    0    0    0]\n",
            "  [   0    0    2 ...    0    0    0]]\n",
            "\n",
            " [[   0    0    0 ...    0    0    0]\n",
            "  [   0    0    0 ...    0    0    0]\n",
            "  [   0    0    0 ...    0    0    0]\n",
            "  ...\n",
            "  [   0    0    0 ...    0    0    0]\n",
            "  [   0    0    0 ...    0    0    0]\n",
            "  [   0    0    0 ...    0    0    0]]]\n",
            "255\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6lMXbqaRSSw",
        "colab_type": "code",
        "outputId": "70b6e3ef-4789-4f6d-b2a3-2ac9a7b51356",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#crop patches\n",
        "patch_size = 3\n",
        "data_patches = get_patches(processed_image, patch_size)\n",
        "print('LBP patches shape: {}'.format(data_patches.shape))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LBP patches shape: (610, 340, 3, 3, 103)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFhQFkJMRzDN",
        "colab_type": "code",
        "outputId": "840e043e-5cb1-42ef-879a-503f1acca69c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "'''\n",
        "Save patches and ground truth into arrays\n",
        "'''\n",
        "\n",
        "# patches\n",
        "patch_arr = np.asarray(arr5D_to_list_of_3D_arr(data_patches), dtype=np.uint8)\n",
        "print('patch_lbp_list len: {}'.format(patch_arr.shape))\n",
        "data_patches = None\n",
        "gc.collect()\n",
        "\n",
        "# ground truth\n",
        "labels_gt = ground_truth.flatten()\n",
        "print(\"Labels len: {}\".format(len(labels_gt)))\n",
        "print(np.unique(labels_gt))\n",
        "ground_truth = None\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "patch_lbp_list len: (207400, 3, 3, 103)\n",
            "Labels len: 207400\n",
            "[0 1 2 3 4 5 6 7 8 9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQRGqq2Ulb_s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Reshape to match LSTM input size\n",
        "'''\n",
        "patches_sequence = patch_arr.reshape(patch_arr.shape[0], patch_arr.shape[1], patch_arr.shape[2] * patch_arr.shape[3])\n",
        "patch_arr = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vei-cPSCR611",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "split data to training and validation datasets\n",
        "'''\n",
        "gc.collect()\n",
        "\n",
        "val_split = 0.25\n",
        "X_train, X_val, y_train, y_val = train_test_split(patches_sequence, labels_gt, test_size=val_split, stratify=labels_gt)\n",
        "\n",
        "#free memory\n",
        "patches_sequence = None\n",
        "label_gt = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPny7qU9RQBl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "RNN LSTM\n",
        "'''\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "# model definition function\n",
        "def create_model_LSTM(l2_loss_lambda = None, input_shape = (7, 7*103)):\n",
        "    keras.backend.clear_session()\n",
        "\n",
        "    # target_size = (32, 32)\n",
        "    l2 = None if l2_loss_lambda is None else keras.regularizers.l2(l2_loss_lambda)\n",
        "    if l2 is not None:\n",
        "        print('Using L2 regularization - l2_loss_lambda = %.4f' % l2_loss_lambda)\n",
        "\n",
        "    model = keras.Sequential(\n",
        "        [\n",
        "            # keras.layers.Lambda(lambda image: tf.image.resize(image, target_size)),\n",
        "            tf.compat.v1.keras.layers.CuDNNLSTM(512, input_shape = input_shape, return_sequences = True),\n",
        "            keras.layers.Dropout(0.2),\n",
        "         \n",
        "            tf.compat.v1.keras.layers.CuDNNLSTM(512),\n",
        "            keras.layers.Dropout(0.2),\n",
        "         \n",
        "            keras.layers.Dense(256, activation=tf.nn.relu, kernel_regularizer=l2),\n",
        "            keras.layers.Dropout(0.2),\n",
        "            \n",
        "            keras.layers.Dense(10, activation=tf.nn.softmax),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # model compiling\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNLGGMCmSRB5",
        "colab_type": "code",
        "outputId": "e7dcfb3c-2f77-48c3-9419-b5f4ee37d7b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "'''\n",
        "Create and fit model, get loss, accuracy and f1-score metrics\n",
        "'''\n",
        "model = create_model_LSTM(None, (patch_size, patch_size*103))\n",
        "\n",
        "gc.collect()\n",
        "history = model.fit(X_train, y_train, batch_size = 256, epochs=300, validation_data=(X_val, y_val))\n",
        "print(model.summary())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "608/608 [==============================] - 13s 21ms/step - loss: 0.8046 - accuracy: 0.7946 - val_loss: 0.7643 - val_accuracy: 0.7966\n",
            "Epoch 2/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.7617 - accuracy: 0.7970 - val_loss: 0.7335 - val_accuracy: 0.7974\n",
            "Epoch 3/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.7454 - accuracy: 0.7975 - val_loss: 0.7253 - val_accuracy: 0.7987\n",
            "Epoch 4/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.7322 - accuracy: 0.7984 - val_loss: 0.7125 - val_accuracy: 0.7999\n",
            "Epoch 5/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.7231 - accuracy: 0.7987 - val_loss: 0.7043 - val_accuracy: 0.7986\n",
            "Epoch 6/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.7149 - accuracy: 0.7992 - val_loss: 0.7017 - val_accuracy: 0.8007\n",
            "Epoch 7/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.7072 - accuracy: 0.8001 - val_loss: 0.6918 - val_accuracy: 0.7995\n",
            "Epoch 8/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.7030 - accuracy: 0.8001 - val_loss: 0.6821 - val_accuracy: 0.8024\n",
            "Epoch 9/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.6934 - accuracy: 0.8010 - val_loss: 0.6805 - val_accuracy: 0.8011\n",
            "Epoch 10/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.6867 - accuracy: 0.8011 - val_loss: 0.6748 - val_accuracy: 0.8033\n",
            "Epoch 11/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.6825 - accuracy: 0.8014 - val_loss: 0.6658 - val_accuracy: 0.8032\n",
            "Epoch 12/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.6761 - accuracy: 0.8021 - val_loss: 0.6597 - val_accuracy: 0.8027\n",
            "Epoch 13/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.6712 - accuracy: 0.8024 - val_loss: 0.6591 - val_accuracy: 0.8030\n",
            "Epoch 14/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.6713 - accuracy: 0.8026 - val_loss: 0.6594 - val_accuracy: 0.8019\n",
            "Epoch 15/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.6648 - accuracy: 0.8034 - val_loss: 0.6549 - val_accuracy: 0.8037\n",
            "Epoch 16/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.6631 - accuracy: 0.8035 - val_loss: 0.6515 - val_accuracy: 0.8039\n",
            "Epoch 17/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.6529 - accuracy: 0.8042 - val_loss: 0.6422 - val_accuracy: 0.8041\n",
            "Epoch 18/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.6540 - accuracy: 0.8041 - val_loss: 0.6476 - val_accuracy: 0.8037\n",
            "Epoch 19/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.6474 - accuracy: 0.8049 - val_loss: 0.6409 - val_accuracy: 0.8046\n",
            "Epoch 20/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.6472 - accuracy: 0.8054 - val_loss: 0.6455 - val_accuracy: 0.8038\n",
            "Epoch 21/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.6464 - accuracy: 0.8056 - val_loss: 0.6450 - val_accuracy: 0.8042\n",
            "Epoch 22/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.6422 - accuracy: 0.8054 - val_loss: 0.6452 - val_accuracy: 0.8030\n",
            "Epoch 23/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.6364 - accuracy: 0.8060 - val_loss: 0.6405 - val_accuracy: 0.8037\n",
            "Epoch 24/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.6322 - accuracy: 0.8074 - val_loss: 0.6394 - val_accuracy: 0.8046\n",
            "Epoch 25/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.6291 - accuracy: 0.8063 - val_loss: 0.6434 - val_accuracy: 0.8035\n",
            "Epoch 26/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.6309 - accuracy: 0.8072 - val_loss: 0.6339 - val_accuracy: 0.8056\n",
            "Epoch 27/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.6228 - accuracy: 0.8075 - val_loss: 0.6339 - val_accuracy: 0.8048\n",
            "Epoch 28/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.6207 - accuracy: 0.8083 - val_loss: 0.6382 - val_accuracy: 0.8049\n",
            "Epoch 29/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.6237 - accuracy: 0.8080 - val_loss: 0.6372 - val_accuracy: 0.8058\n",
            "Epoch 30/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.6175 - accuracy: 0.8091 - val_loss: 0.6283 - val_accuracy: 0.8057\n",
            "Epoch 31/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.6138 - accuracy: 0.8094 - val_loss: 0.6273 - val_accuracy: 0.8068\n",
            "Epoch 32/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.6096 - accuracy: 0.8100 - val_loss: 0.6311 - val_accuracy: 0.8066\n",
            "Epoch 33/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.6078 - accuracy: 0.8100 - val_loss: 0.6335 - val_accuracy: 0.8050\n",
            "Epoch 34/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.6101 - accuracy: 0.8104 - val_loss: 0.6261 - val_accuracy: 0.8065\n",
            "Epoch 35/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.6039 - accuracy: 0.8110 - val_loss: 0.6267 - val_accuracy: 0.8053\n",
            "Epoch 36/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5983 - accuracy: 0.8116 - val_loss: 0.6231 - val_accuracy: 0.8072\n",
            "Epoch 37/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5987 - accuracy: 0.8113 - val_loss: 0.6236 - val_accuracy: 0.8059\n",
            "Epoch 38/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5972 - accuracy: 0.8114 - val_loss: 0.6266 - val_accuracy: 0.8064\n",
            "Epoch 39/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5928 - accuracy: 0.8127 - val_loss: 0.6243 - val_accuracy: 0.8071\n",
            "Epoch 40/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5919 - accuracy: 0.8125 - val_loss: 0.6182 - val_accuracy: 0.8062\n",
            "Epoch 41/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5886 - accuracy: 0.8132 - val_loss: 0.6189 - val_accuracy: 0.8065\n",
            "Epoch 42/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5848 - accuracy: 0.8129 - val_loss: 0.6197 - val_accuracy: 0.8067\n",
            "Epoch 43/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5849 - accuracy: 0.8137 - val_loss: 0.6240 - val_accuracy: 0.8072\n",
            "Epoch 44/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.5826 - accuracy: 0.8136 - val_loss: 0.6251 - val_accuracy: 0.8072\n",
            "Epoch 45/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5807 - accuracy: 0.8144 - val_loss: 0.6278 - val_accuracy: 0.8066\n",
            "Epoch 46/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5786 - accuracy: 0.8144 - val_loss: 0.6240 - val_accuracy: 0.8076\n",
            "Epoch 47/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5812 - accuracy: 0.8143 - val_loss: 0.6273 - val_accuracy: 0.8075\n",
            "Epoch 48/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5804 - accuracy: 0.8148 - val_loss: 0.6196 - val_accuracy: 0.8069\n",
            "Epoch 49/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.5771 - accuracy: 0.8149 - val_loss: 0.6157 - val_accuracy: 0.8073\n",
            "Epoch 50/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5729 - accuracy: 0.8156 - val_loss: 0.6158 - val_accuracy: 0.8083\n",
            "Epoch 51/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5702 - accuracy: 0.8163 - val_loss: 0.6224 - val_accuracy: 0.8080\n",
            "Epoch 52/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5706 - accuracy: 0.8163 - val_loss: 0.6247 - val_accuracy: 0.8064\n",
            "Epoch 53/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5734 - accuracy: 0.8158 - val_loss: 0.6236 - val_accuracy: 0.8057\n",
            "Epoch 54/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5712 - accuracy: 0.8152 - val_loss: 0.6200 - val_accuracy: 0.8064\n",
            "Epoch 55/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5684 - accuracy: 0.8170 - val_loss: 0.6234 - val_accuracy: 0.8080\n",
            "Epoch 56/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5621 - accuracy: 0.8167 - val_loss: 0.6151 - val_accuracy: 0.8094\n",
            "Epoch 57/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5606 - accuracy: 0.8172 - val_loss: 0.6200 - val_accuracy: 0.8081\n",
            "Epoch 58/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5604 - accuracy: 0.8170 - val_loss: 0.6196 - val_accuracy: 0.8081\n",
            "Epoch 59/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.5594 - accuracy: 0.8178 - val_loss: 0.6200 - val_accuracy: 0.8073\n",
            "Epoch 60/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.5595 - accuracy: 0.8189 - val_loss: 0.6192 - val_accuracy: 0.8074\n",
            "Epoch 61/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.5568 - accuracy: 0.8183 - val_loss: 0.6178 - val_accuracy: 0.8077\n",
            "Epoch 62/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5574 - accuracy: 0.8183 - val_loss: 0.6196 - val_accuracy: 0.8074\n",
            "Epoch 63/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5562 - accuracy: 0.8189 - val_loss: 0.6234 - val_accuracy: 0.8048\n",
            "Epoch 64/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5527 - accuracy: 0.8190 - val_loss: 0.6197 - val_accuracy: 0.8077\n",
            "Epoch 65/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5523 - accuracy: 0.8194 - val_loss: 0.6226 - val_accuracy: 0.8085\n",
            "Epoch 66/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5528 - accuracy: 0.8190 - val_loss: 0.6199 - val_accuracy: 0.8058\n",
            "Epoch 67/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5452 - accuracy: 0.8208 - val_loss: 0.6195 - val_accuracy: 0.8060\n",
            "Epoch 68/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5459 - accuracy: 0.8202 - val_loss: 0.6192 - val_accuracy: 0.8088\n",
            "Epoch 69/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5446 - accuracy: 0.8202 - val_loss: 0.6209 - val_accuracy: 0.8066\n",
            "Epoch 70/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5468 - accuracy: 0.8207 - val_loss: 0.6211 - val_accuracy: 0.8073\n",
            "Epoch 71/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5455 - accuracy: 0.8212 - val_loss: 0.6274 - val_accuracy: 0.8080\n",
            "Epoch 72/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5460 - accuracy: 0.8207 - val_loss: 0.6230 - val_accuracy: 0.8077\n",
            "Epoch 73/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5429 - accuracy: 0.8213 - val_loss: 0.6204 - val_accuracy: 0.8061\n",
            "Epoch 74/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5405 - accuracy: 0.8216 - val_loss: 0.6209 - val_accuracy: 0.8080\n",
            "Epoch 75/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5407 - accuracy: 0.8219 - val_loss: 0.6206 - val_accuracy: 0.8065\n",
            "Epoch 76/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5369 - accuracy: 0.8223 - val_loss: 0.6228 - val_accuracy: 0.8080\n",
            "Epoch 77/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5372 - accuracy: 0.8226 - val_loss: 0.6276 - val_accuracy: 0.8079\n",
            "Epoch 78/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5385 - accuracy: 0.8226 - val_loss: 0.6192 - val_accuracy: 0.8081\n",
            "Epoch 79/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5359 - accuracy: 0.8230 - val_loss: 0.6242 - val_accuracy: 0.8064\n",
            "Epoch 80/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5344 - accuracy: 0.8237 - val_loss: 0.6276 - val_accuracy: 0.8088\n",
            "Epoch 81/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5349 - accuracy: 0.8235 - val_loss: 0.6208 - val_accuracy: 0.8087\n",
            "Epoch 82/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5380 - accuracy: 0.8226 - val_loss: 0.6297 - val_accuracy: 0.8082\n",
            "Epoch 83/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5344 - accuracy: 0.8235 - val_loss: 0.6192 - val_accuracy: 0.8087\n",
            "Epoch 84/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5251 - accuracy: 0.8240 - val_loss: 0.6228 - val_accuracy: 0.8081\n",
            "Epoch 85/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5251 - accuracy: 0.8252 - val_loss: 0.6237 - val_accuracy: 0.8075\n",
            "Epoch 86/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5281 - accuracy: 0.8249 - val_loss: 0.6327 - val_accuracy: 0.8079\n",
            "Epoch 87/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5311 - accuracy: 0.8244 - val_loss: 0.6295 - val_accuracy: 0.8088\n",
            "Epoch 88/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5258 - accuracy: 0.8258 - val_loss: 0.6250 - val_accuracy: 0.8077\n",
            "Epoch 89/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5263 - accuracy: 0.8254 - val_loss: 0.6277 - val_accuracy: 0.8079\n",
            "Epoch 90/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5248 - accuracy: 0.8250 - val_loss: 0.6226 - val_accuracy: 0.8096\n",
            "Epoch 91/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5276 - accuracy: 0.8256 - val_loss: 0.6297 - val_accuracy: 0.8067\n",
            "Epoch 92/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5263 - accuracy: 0.8257 - val_loss: 0.6256 - val_accuracy: 0.8067\n",
            "Epoch 93/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5236 - accuracy: 0.8257 - val_loss: 0.6282 - val_accuracy: 0.8082\n",
            "Epoch 94/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5212 - accuracy: 0.8269 - val_loss: 0.6241 - val_accuracy: 0.8084\n",
            "Epoch 95/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5156 - accuracy: 0.8279 - val_loss: 0.6273 - val_accuracy: 0.8076\n",
            "Epoch 96/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.5195 - accuracy: 0.8263 - val_loss: 0.6326 - val_accuracy: 0.8091\n",
            "Epoch 97/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5211 - accuracy: 0.8265 - val_loss: 0.6297 - val_accuracy: 0.8074\n",
            "Epoch 98/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5192 - accuracy: 0.8264 - val_loss: 0.6270 - val_accuracy: 0.8089\n",
            "Epoch 99/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5199 - accuracy: 0.8271 - val_loss: 0.6269 - val_accuracy: 0.8079\n",
            "Epoch 100/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5186 - accuracy: 0.8270 - val_loss: 0.6344 - val_accuracy: 0.8060\n",
            "Epoch 101/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5170 - accuracy: 0.8284 - val_loss: 0.6339 - val_accuracy: 0.8065\n",
            "Epoch 102/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5102 - accuracy: 0.8294 - val_loss: 0.6298 - val_accuracy: 0.8078\n",
            "Epoch 103/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.5109 - accuracy: 0.8291 - val_loss: 0.6331 - val_accuracy: 0.8088\n",
            "Epoch 104/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.5089 - accuracy: 0.8293 - val_loss: 0.6312 - val_accuracy: 0.8083\n",
            "Epoch 105/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5082 - accuracy: 0.8303 - val_loss: 0.6285 - val_accuracy: 0.8060\n",
            "Epoch 106/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5090 - accuracy: 0.8295 - val_loss: 0.6254 - val_accuracy: 0.8075\n",
            "Epoch 107/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5054 - accuracy: 0.8305 - val_loss: 0.6312 - val_accuracy: 0.8084\n",
            "Epoch 108/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5082 - accuracy: 0.8300 - val_loss: 0.6272 - val_accuracy: 0.8076\n",
            "Epoch 109/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5083 - accuracy: 0.8297 - val_loss: 0.6370 - val_accuracy: 0.8045\n",
            "Epoch 110/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5077 - accuracy: 0.8291 - val_loss: 0.6284 - val_accuracy: 0.8065\n",
            "Epoch 111/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5046 - accuracy: 0.8300 - val_loss: 0.6284 - val_accuracy: 0.8082\n",
            "Epoch 112/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.5024 - accuracy: 0.8304 - val_loss: 0.6347 - val_accuracy: 0.8074\n",
            "Epoch 113/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5028 - accuracy: 0.8310 - val_loss: 0.6322 - val_accuracy: 0.8067\n",
            "Epoch 114/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5013 - accuracy: 0.8308 - val_loss: 0.6322 - val_accuracy: 0.8075\n",
            "Epoch 115/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5007 - accuracy: 0.8313 - val_loss: 0.6306 - val_accuracy: 0.8077\n",
            "Epoch 116/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5031 - accuracy: 0.8306 - val_loss: 0.6352 - val_accuracy: 0.8063\n",
            "Epoch 117/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.5065 - accuracy: 0.8304 - val_loss: 0.6352 - val_accuracy: 0.8057\n",
            "Epoch 118/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4987 - accuracy: 0.8310 - val_loss: 0.6323 - val_accuracy: 0.8096\n",
            "Epoch 119/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4985 - accuracy: 0.8321 - val_loss: 0.6367 - val_accuracy: 0.8078\n",
            "Epoch 120/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4995 - accuracy: 0.8314 - val_loss: 0.6355 - val_accuracy: 0.8074\n",
            "Epoch 121/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.5007 - accuracy: 0.8318 - val_loss: 0.6385 - val_accuracy: 0.8078\n",
            "Epoch 122/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4921 - accuracy: 0.8328 - val_loss: 0.6352 - val_accuracy: 0.8080\n",
            "Epoch 123/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4918 - accuracy: 0.8324 - val_loss: 0.6388 - val_accuracy: 0.8090\n",
            "Epoch 124/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4911 - accuracy: 0.8328 - val_loss: 0.6391 - val_accuracy: 0.8045\n",
            "Epoch 125/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4941 - accuracy: 0.8332 - val_loss: 0.6402 - val_accuracy: 0.8067\n",
            "Epoch 126/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4970 - accuracy: 0.8315 - val_loss: 0.6383 - val_accuracy: 0.8061\n",
            "Epoch 127/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4930 - accuracy: 0.8326 - val_loss: 0.6326 - val_accuracy: 0.8071\n",
            "Epoch 128/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4915 - accuracy: 0.8328 - val_loss: 0.6345 - val_accuracy: 0.8076\n",
            "Epoch 129/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4896 - accuracy: 0.8343 - val_loss: 0.6362 - val_accuracy: 0.8059\n",
            "Epoch 130/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4894 - accuracy: 0.8331 - val_loss: 0.6346 - val_accuracy: 0.8073\n",
            "Epoch 131/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4901 - accuracy: 0.8343 - val_loss: 0.6372 - val_accuracy: 0.8072\n",
            "Epoch 132/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4863 - accuracy: 0.8346 - val_loss: 0.6353 - val_accuracy: 0.8077\n",
            "Epoch 133/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4860 - accuracy: 0.8344 - val_loss: 0.6382 - val_accuracy: 0.8084\n",
            "Epoch 134/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4836 - accuracy: 0.8347 - val_loss: 0.6411 - val_accuracy: 0.8076\n",
            "Epoch 135/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4843 - accuracy: 0.8354 - val_loss: 0.6380 - val_accuracy: 0.8030\n",
            "Epoch 136/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4864 - accuracy: 0.8341 - val_loss: 0.6428 - val_accuracy: 0.8081\n",
            "Epoch 137/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4896 - accuracy: 0.8345 - val_loss: 0.6382 - val_accuracy: 0.8074\n",
            "Epoch 138/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4867 - accuracy: 0.8351 - val_loss: 0.6506 - val_accuracy: 0.8082\n",
            "Epoch 139/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4864 - accuracy: 0.8345 - val_loss: 0.6421 - val_accuracy: 0.8064\n",
            "Epoch 140/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4843 - accuracy: 0.8345 - val_loss: 0.6418 - val_accuracy: 0.8077\n",
            "Epoch 141/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4832 - accuracy: 0.8356 - val_loss: 0.6390 - val_accuracy: 0.8074\n",
            "Epoch 142/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4801 - accuracy: 0.8365 - val_loss: 0.6404 - val_accuracy: 0.8081\n",
            "Epoch 143/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4770 - accuracy: 0.8374 - val_loss: 0.6441 - val_accuracy: 0.8068\n",
            "Epoch 144/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4796 - accuracy: 0.8363 - val_loss: 0.6433 - val_accuracy: 0.8069\n",
            "Epoch 145/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4760 - accuracy: 0.8371 - val_loss: 0.6477 - val_accuracy: 0.8079\n",
            "Epoch 146/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4785 - accuracy: 0.8373 - val_loss: 0.6441 - val_accuracy: 0.8056\n",
            "Epoch 147/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4781 - accuracy: 0.8358 - val_loss: 0.6494 - val_accuracy: 0.8080\n",
            "Epoch 148/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4810 - accuracy: 0.8369 - val_loss: 0.6504 - val_accuracy: 0.8056\n",
            "Epoch 149/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4825 - accuracy: 0.8361 - val_loss: 0.6462 - val_accuracy: 0.8086\n",
            "Epoch 150/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4818 - accuracy: 0.8362 - val_loss: 0.6469 - val_accuracy: 0.8060\n",
            "Epoch 151/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4769 - accuracy: 0.8371 - val_loss: 0.6432 - val_accuracy: 0.8081\n",
            "Epoch 152/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4773 - accuracy: 0.8376 - val_loss: 0.6491 - val_accuracy: 0.8072\n",
            "Epoch 153/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4797 - accuracy: 0.8371 - val_loss: 0.6481 - val_accuracy: 0.8073\n",
            "Epoch 154/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4792 - accuracy: 0.8371 - val_loss: 0.6410 - val_accuracy: 0.8078\n",
            "Epoch 155/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4788 - accuracy: 0.8380 - val_loss: 0.6508 - val_accuracy: 0.8079\n",
            "Epoch 156/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4745 - accuracy: 0.8381 - val_loss: 0.6406 - val_accuracy: 0.8078\n",
            "Epoch 157/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4751 - accuracy: 0.8378 - val_loss: 0.6457 - val_accuracy: 0.8075\n",
            "Epoch 158/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4762 - accuracy: 0.8385 - val_loss: 0.6453 - val_accuracy: 0.8066\n",
            "Epoch 159/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4752 - accuracy: 0.8379 - val_loss: 0.6436 - val_accuracy: 0.8076\n",
            "Epoch 160/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4698 - accuracy: 0.8392 - val_loss: 0.6514 - val_accuracy: 0.8078\n",
            "Epoch 161/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4697 - accuracy: 0.8395 - val_loss: 0.6506 - val_accuracy: 0.8083\n",
            "Epoch 162/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4723 - accuracy: 0.8387 - val_loss: 0.6506 - val_accuracy: 0.8059\n",
            "Epoch 163/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4731 - accuracy: 0.8388 - val_loss: 0.6589 - val_accuracy: 0.8073\n",
            "Epoch 164/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4730 - accuracy: 0.8384 - val_loss: 0.6528 - val_accuracy: 0.8069\n",
            "Epoch 165/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4684 - accuracy: 0.8396 - val_loss: 0.6505 - val_accuracy: 0.8073\n",
            "Epoch 166/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4635 - accuracy: 0.8402 - val_loss: 0.6492 - val_accuracy: 0.8076\n",
            "Epoch 167/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4676 - accuracy: 0.8397 - val_loss: 0.6549 - val_accuracy: 0.8065\n",
            "Epoch 168/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4647 - accuracy: 0.8402 - val_loss: 0.6519 - val_accuracy: 0.8077\n",
            "Epoch 169/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4648 - accuracy: 0.8405 - val_loss: 0.6559 - val_accuracy: 0.8065\n",
            "Epoch 170/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4640 - accuracy: 0.8415 - val_loss: 0.6510 - val_accuracy: 0.8075\n",
            "Epoch 171/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4659 - accuracy: 0.8406 - val_loss: 0.6501 - val_accuracy: 0.8089\n",
            "Epoch 172/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4688 - accuracy: 0.8404 - val_loss: 0.6538 - val_accuracy: 0.8083\n",
            "Epoch 173/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4661 - accuracy: 0.8403 - val_loss: 0.6504 - val_accuracy: 0.8055\n",
            "Epoch 174/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4662 - accuracy: 0.8406 - val_loss: 0.6481 - val_accuracy: 0.8078\n",
            "Epoch 175/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4629 - accuracy: 0.8416 - val_loss: 0.6501 - val_accuracy: 0.8076\n",
            "Epoch 176/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4649 - accuracy: 0.8408 - val_loss: 0.6484 - val_accuracy: 0.8079\n",
            "Epoch 177/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4612 - accuracy: 0.8416 - val_loss: 0.6484 - val_accuracy: 0.8064\n",
            "Epoch 178/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4647 - accuracy: 0.8414 - val_loss: 0.6498 - val_accuracy: 0.8054\n",
            "Epoch 179/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4629 - accuracy: 0.8412 - val_loss: 0.6549 - val_accuracy: 0.8082\n",
            "Epoch 180/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4641 - accuracy: 0.8409 - val_loss: 0.6546 - val_accuracy: 0.8086\n",
            "Epoch 181/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4633 - accuracy: 0.8416 - val_loss: 0.6496 - val_accuracy: 0.8079\n",
            "Epoch 182/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4591 - accuracy: 0.8428 - val_loss: 0.6554 - val_accuracy: 0.8072\n",
            "Epoch 183/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4622 - accuracy: 0.8422 - val_loss: 0.6554 - val_accuracy: 0.8055\n",
            "Epoch 184/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4641 - accuracy: 0.8419 - val_loss: 0.6529 - val_accuracy: 0.8071\n",
            "Epoch 185/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4608 - accuracy: 0.8422 - val_loss: 0.6518 - val_accuracy: 0.8065\n",
            "Epoch 186/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4595 - accuracy: 0.8429 - val_loss: 0.6483 - val_accuracy: 0.8086\n",
            "Epoch 187/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4562 - accuracy: 0.8435 - val_loss: 0.6480 - val_accuracy: 0.8073\n",
            "Epoch 188/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4535 - accuracy: 0.8438 - val_loss: 0.6516 - val_accuracy: 0.8066\n",
            "Epoch 189/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4578 - accuracy: 0.8416 - val_loss: 0.6617 - val_accuracy: 0.8078\n",
            "Epoch 190/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4521 - accuracy: 0.8438 - val_loss: 0.6494 - val_accuracy: 0.8078\n",
            "Epoch 191/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4539 - accuracy: 0.8432 - val_loss: 0.6618 - val_accuracy: 0.8080\n",
            "Epoch 192/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4542 - accuracy: 0.8437 - val_loss: 0.6637 - val_accuracy: 0.8076\n",
            "Epoch 193/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4603 - accuracy: 0.8433 - val_loss: 0.6619 - val_accuracy: 0.8069\n",
            "Epoch 194/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4540 - accuracy: 0.8448 - val_loss: 0.6571 - val_accuracy: 0.8068\n",
            "Epoch 195/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4532 - accuracy: 0.8435 - val_loss: 0.6582 - val_accuracy: 0.8081\n",
            "Epoch 196/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4539 - accuracy: 0.8439 - val_loss: 0.6605 - val_accuracy: 0.8059\n",
            "Epoch 197/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4504 - accuracy: 0.8450 - val_loss: 0.6646 - val_accuracy: 0.8082\n",
            "Epoch 198/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4488 - accuracy: 0.8455 - val_loss: 0.6583 - val_accuracy: 0.8082\n",
            "Epoch 199/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4481 - accuracy: 0.8458 - val_loss: 0.6618 - val_accuracy: 0.8059\n",
            "Epoch 200/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4514 - accuracy: 0.8452 - val_loss: 0.6632 - val_accuracy: 0.8070\n",
            "Epoch 201/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4488 - accuracy: 0.8453 - val_loss: 0.6606 - val_accuracy: 0.8077\n",
            "Epoch 202/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4464 - accuracy: 0.8460 - val_loss: 0.6675 - val_accuracy: 0.8071\n",
            "Epoch 203/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4458 - accuracy: 0.8459 - val_loss: 0.6594 - val_accuracy: 0.8075\n",
            "Epoch 204/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4472 - accuracy: 0.8454 - val_loss: 0.6682 - val_accuracy: 0.8068\n",
            "Epoch 205/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4524 - accuracy: 0.8449 - val_loss: 0.6578 - val_accuracy: 0.8070\n",
            "Epoch 206/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4528 - accuracy: 0.8435 - val_loss: 0.6546 - val_accuracy: 0.8078\n",
            "Epoch 207/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4511 - accuracy: 0.8448 - val_loss: 0.6556 - val_accuracy: 0.8063\n",
            "Epoch 208/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4489 - accuracy: 0.8451 - val_loss: 0.6583 - val_accuracy: 0.8061\n",
            "Epoch 209/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4478 - accuracy: 0.8462 - val_loss: 0.6593 - val_accuracy: 0.8076\n",
            "Epoch 210/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4514 - accuracy: 0.8453 - val_loss: 0.6577 - val_accuracy: 0.8089\n",
            "Epoch 211/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4433 - accuracy: 0.8472 - val_loss: 0.6595 - val_accuracy: 0.8067\n",
            "Epoch 212/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4413 - accuracy: 0.8476 - val_loss: 0.6566 - val_accuracy: 0.8055\n",
            "Epoch 213/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4421 - accuracy: 0.8462 - val_loss: 0.6649 - val_accuracy: 0.8059\n",
            "Epoch 214/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4429 - accuracy: 0.8475 - val_loss: 0.6566 - val_accuracy: 0.8070\n",
            "Epoch 215/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4402 - accuracy: 0.8479 - val_loss: 0.6595 - val_accuracy: 0.8062\n",
            "Epoch 216/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4446 - accuracy: 0.8469 - val_loss: 0.6596 - val_accuracy: 0.8064\n",
            "Epoch 217/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4482 - accuracy: 0.8466 - val_loss: 0.6674 - val_accuracy: 0.8085\n",
            "Epoch 218/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4459 - accuracy: 0.8462 - val_loss: 0.6606 - val_accuracy: 0.8083\n",
            "Epoch 219/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4424 - accuracy: 0.8472 - val_loss: 0.6671 - val_accuracy: 0.8092\n",
            "Epoch 220/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4470 - accuracy: 0.8464 - val_loss: 0.6590 - val_accuracy: 0.8075\n",
            "Epoch 221/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4471 - accuracy: 0.8459 - val_loss: 0.6613 - val_accuracy: 0.8077\n",
            "Epoch 222/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4448 - accuracy: 0.8476 - val_loss: 0.6638 - val_accuracy: 0.8071\n",
            "Epoch 223/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4429 - accuracy: 0.8474 - val_loss: 0.6616 - val_accuracy: 0.8087\n",
            "Epoch 224/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4436 - accuracy: 0.8464 - val_loss: 0.6735 - val_accuracy: 0.8055\n",
            "Epoch 225/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4440 - accuracy: 0.8467 - val_loss: 0.6636 - val_accuracy: 0.8066\n",
            "Epoch 226/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4408 - accuracy: 0.8485 - val_loss: 0.6708 - val_accuracy: 0.8073\n",
            "Epoch 227/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4425 - accuracy: 0.8476 - val_loss: 0.6746 - val_accuracy: 0.8062\n",
            "Epoch 228/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4439 - accuracy: 0.8470 - val_loss: 0.6611 - val_accuracy: 0.8072\n",
            "Epoch 229/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4402 - accuracy: 0.8476 - val_loss: 0.6583 - val_accuracy: 0.8062\n",
            "Epoch 230/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4396 - accuracy: 0.8478 - val_loss: 0.6637 - val_accuracy: 0.8074\n",
            "Epoch 231/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4333 - accuracy: 0.8496 - val_loss: 0.6670 - val_accuracy: 0.8086\n",
            "Epoch 232/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4375 - accuracy: 0.8471 - val_loss: 0.6673 - val_accuracy: 0.8079\n",
            "Epoch 233/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4375 - accuracy: 0.8483 - val_loss: 0.6642 - val_accuracy: 0.8089\n",
            "Epoch 234/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4358 - accuracy: 0.8495 - val_loss: 0.6680 - val_accuracy: 0.8083\n",
            "Epoch 235/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4382 - accuracy: 0.8481 - val_loss: 0.6593 - val_accuracy: 0.8071\n",
            "Epoch 236/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4362 - accuracy: 0.8492 - val_loss: 0.6654 - val_accuracy: 0.8059\n",
            "Epoch 237/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4316 - accuracy: 0.8493 - val_loss: 0.6665 - val_accuracy: 0.8065\n",
            "Epoch 238/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4358 - accuracy: 0.8489 - val_loss: 0.6700 - val_accuracy: 0.8051\n",
            "Epoch 239/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4337 - accuracy: 0.8504 - val_loss: 0.6653 - val_accuracy: 0.8054\n",
            "Epoch 240/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4314 - accuracy: 0.8503 - val_loss: 0.6700 - val_accuracy: 0.8067\n",
            "Epoch 241/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4306 - accuracy: 0.8504 - val_loss: 0.6729 - val_accuracy: 0.8062\n",
            "Epoch 242/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4301 - accuracy: 0.8501 - val_loss: 0.6683 - val_accuracy: 0.8065\n",
            "Epoch 243/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4309 - accuracy: 0.8502 - val_loss: 0.6682 - val_accuracy: 0.8074\n",
            "Epoch 244/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4279 - accuracy: 0.8514 - val_loss: 0.6666 - val_accuracy: 0.8068\n",
            "Epoch 245/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4345 - accuracy: 0.8505 - val_loss: 0.6636 - val_accuracy: 0.8086\n",
            "Epoch 246/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4306 - accuracy: 0.8507 - val_loss: 0.6760 - val_accuracy: 0.8067\n",
            "Epoch 247/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4256 - accuracy: 0.8519 - val_loss: 0.6725 - val_accuracy: 0.8054\n",
            "Epoch 248/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4254 - accuracy: 0.8511 - val_loss: 0.6750 - val_accuracy: 0.8059\n",
            "Epoch 249/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4286 - accuracy: 0.8518 - val_loss: 0.6751 - val_accuracy: 0.8060\n",
            "Epoch 250/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4253 - accuracy: 0.8522 - val_loss: 0.6790 - val_accuracy: 0.8061\n",
            "Epoch 251/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4226 - accuracy: 0.8539 - val_loss: 0.6763 - val_accuracy: 0.8074\n",
            "Epoch 252/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4276 - accuracy: 0.8514 - val_loss: 0.6657 - val_accuracy: 0.8060\n",
            "Epoch 253/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4271 - accuracy: 0.8516 - val_loss: 0.6724 - val_accuracy: 0.8074\n",
            "Epoch 254/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4281 - accuracy: 0.8518 - val_loss: 0.6715 - val_accuracy: 0.8057\n",
            "Epoch 255/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4248 - accuracy: 0.8520 - val_loss: 0.6759 - val_accuracy: 0.8065\n",
            "Epoch 256/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4277 - accuracy: 0.8516 - val_loss: 0.6691 - val_accuracy: 0.8082\n",
            "Epoch 257/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4255 - accuracy: 0.8521 - val_loss: 0.6730 - val_accuracy: 0.8079\n",
            "Epoch 258/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4254 - accuracy: 0.8521 - val_loss: 0.6729 - val_accuracy: 0.8073\n",
            "Epoch 259/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4267 - accuracy: 0.8518 - val_loss: 0.6770 - val_accuracy: 0.8076\n",
            "Epoch 260/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4280 - accuracy: 0.8514 - val_loss: 0.6755 - val_accuracy: 0.8058\n",
            "Epoch 261/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4284 - accuracy: 0.8526 - val_loss: 0.6696 - val_accuracy: 0.8076\n",
            "Epoch 262/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4316 - accuracy: 0.8510 - val_loss: 0.6745 - val_accuracy: 0.8064\n",
            "Epoch 263/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4261 - accuracy: 0.8526 - val_loss: 0.6768 - val_accuracy: 0.8069\n",
            "Epoch 264/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4273 - accuracy: 0.8527 - val_loss: 0.6780 - val_accuracy: 0.8074\n",
            "Epoch 265/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4273 - accuracy: 0.8521 - val_loss: 0.6753 - val_accuracy: 0.8067\n",
            "Epoch 266/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4261 - accuracy: 0.8536 - val_loss: 0.6767 - val_accuracy: 0.8060\n",
            "Epoch 267/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4259 - accuracy: 0.8525 - val_loss: 0.6680 - val_accuracy: 0.8046\n",
            "Epoch 268/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4329 - accuracy: 0.8498 - val_loss: 0.6701 - val_accuracy: 0.8045\n",
            "Epoch 269/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4292 - accuracy: 0.8519 - val_loss: 0.6767 - val_accuracy: 0.8082\n",
            "Epoch 270/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4251 - accuracy: 0.8522 - val_loss: 0.6722 - val_accuracy: 0.8067\n",
            "Epoch 271/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4243 - accuracy: 0.8530 - val_loss: 0.6780 - val_accuracy: 0.8063\n",
            "Epoch 272/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4327 - accuracy: 0.8511 - val_loss: 0.6801 - val_accuracy: 0.8075\n",
            "Epoch 273/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4302 - accuracy: 0.8517 - val_loss: 0.6709 - val_accuracy: 0.8059\n",
            "Epoch 274/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4327 - accuracy: 0.8515 - val_loss: 0.6763 - val_accuracy: 0.8027\n",
            "Epoch 275/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4345 - accuracy: 0.8508 - val_loss: 0.6830 - val_accuracy: 0.8057\n",
            "Epoch 276/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4313 - accuracy: 0.8512 - val_loss: 0.6734 - val_accuracy: 0.8068\n",
            "Epoch 277/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4280 - accuracy: 0.8526 - val_loss: 0.6806 - val_accuracy: 0.8057\n",
            "Epoch 278/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4316 - accuracy: 0.8516 - val_loss: 0.6736 - val_accuracy: 0.8068\n",
            "Epoch 279/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4284 - accuracy: 0.8526 - val_loss: 0.6819 - val_accuracy: 0.8049\n",
            "Epoch 280/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4235 - accuracy: 0.8528 - val_loss: 0.6783 - val_accuracy: 0.8075\n",
            "Epoch 281/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4233 - accuracy: 0.8530 - val_loss: 0.6761 - val_accuracy: 0.8067\n",
            "Epoch 282/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4236 - accuracy: 0.8537 - val_loss: 0.6763 - val_accuracy: 0.8070\n",
            "Epoch 283/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4272 - accuracy: 0.8528 - val_loss: 0.6783 - val_accuracy: 0.8064\n",
            "Epoch 284/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4254 - accuracy: 0.8534 - val_loss: 0.6764 - val_accuracy: 0.8072\n",
            "Epoch 285/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4328 - accuracy: 0.8513 - val_loss: 0.6763 - val_accuracy: 0.8050\n",
            "Epoch 286/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4286 - accuracy: 0.8527 - val_loss: 0.6791 - val_accuracy: 0.8065\n",
            "Epoch 287/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4253 - accuracy: 0.8521 - val_loss: 0.6807 - val_accuracy: 0.8066\n",
            "Epoch 288/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4245 - accuracy: 0.8526 - val_loss: 0.6835 - val_accuracy: 0.8034\n",
            "Epoch 289/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4234 - accuracy: 0.8539 - val_loss: 0.6827 - val_accuracy: 0.8059\n",
            "Epoch 290/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4269 - accuracy: 0.8522 - val_loss: 0.6928 - val_accuracy: 0.8068\n",
            "Epoch 291/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4250 - accuracy: 0.8529 - val_loss: 0.6851 - val_accuracy: 0.8055\n",
            "Epoch 292/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4208 - accuracy: 0.8539 - val_loss: 0.6899 - val_accuracy: 0.8065\n",
            "Epoch 293/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4233 - accuracy: 0.8539 - val_loss: 0.6814 - val_accuracy: 0.8064\n",
            "Epoch 294/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4280 - accuracy: 0.8517 - val_loss: 0.6835 - val_accuracy: 0.8073\n",
            "Epoch 295/300\n",
            "608/608 [==============================] - 5s 8ms/step - loss: 0.4243 - accuracy: 0.8533 - val_loss: 0.6843 - val_accuracy: 0.8061\n",
            "Epoch 296/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4252 - accuracy: 0.8535 - val_loss: 0.6877 - val_accuracy: 0.8073\n",
            "Epoch 297/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4264 - accuracy: 0.8528 - val_loss: 0.6813 - val_accuracy: 0.8075\n",
            "Epoch 298/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4235 - accuracy: 0.8532 - val_loss: 0.6792 - val_accuracy: 0.8052\n",
            "Epoch 299/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4211 - accuracy: 0.8539 - val_loss: 0.6852 - val_accuracy: 0.8081\n",
            "Epoch 300/300\n",
            "608/608 [==============================] - 5s 9ms/step - loss: 0.4213 - accuracy: 0.8540 - val_loss: 0.6819 - val_accuracy: 0.8060\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "cu_dnnlstm (CuDNNLSTM)       (None, 3, 512)            1685504   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 3, 512)            0         \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_1 (CuDNNLSTM)     (None, 512)               2101248   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 3,920,650\n",
            "Trainable params: 3,920,650\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKVY1RU10fZo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "872693f8-4e06-4ed9-ba24-685d7fe12c5b"
      },
      "source": [
        "'''\n",
        "Plot accuracy\n",
        "'''\n",
        "\n",
        "plt.figure(1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.legend(['train accuracy', 'val accuracy'])\n",
        "plt.title(\"LBP, LSTM, {}x{} patch\".format(patch_size, patch_size))\n",
        "plt.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7FBRIr29Bv-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "get predictions and predicted labels\n",
        "'''\n",
        "\n",
        "predictions = model.predict(X_val)\n",
        "gt_predicted = [np.argmax(prediction) for prediction in predictions]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLkXzHZBn0E3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "e23d68b9-6972-4676-eb63-574dc33f9cd6"
      },
      "source": [
        "'''\n",
        "Calculate kappa coefictient\n",
        "'''\n",
        "\n",
        "from sklearn.metrics import cohen_kappa_score, f1_score\n",
        "\n",
        "kappa = cohen_kappa_score(y_val, np.array(gt_predicted))\n",
        "f1 = f1_score(y_val, np.array(gt_predicted), average = 'weighted')\n",
        "print(\"Training accuracy: {}\\nValidation accuracy: {}\\nKappa: {}\\nF1: {}\".format(np.max(history.history['accuracy']), np.max(history.history['val_accuracy']),kappa, f1))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training accuracy: 0.8540019392967224\n",
            "Validation accuracy: 0.8095853328704834\n",
            "Kappa: 0.2578562701577345\n",
            "F1: 0.7624128573795643\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}