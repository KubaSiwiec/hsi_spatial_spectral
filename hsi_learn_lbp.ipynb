{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hsi_learn_lbp.ipynb",
      "provenance": [],
      "mount_file_id": "1Q5M1pnRzOaXu-p9oilJghoOflZnRMvFw",
      "authorship_tag": "ABX9TyO/Hk5/DeSEMiyCfOq4g8s2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KubaSiwiec/hsi_spatial_spectral/blob/collab/hsi_learn_lbp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sXnDmXR7RDr2",
        "outputId": "7f813806-cb43-4d10-f47f-6b3657695d42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dWXpdkY4FNz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scipy.io\n",
        "from skimage.util import view_as_windows\n",
        "from skimage.util import pad\n",
        "import numpy as np\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "\n",
        "def get_data(file_name):\n",
        "    return scipy.io.loadmat(file_name)\n",
        "\n",
        "def get_patches(image, size = 3, use_padding = 'SAME'):\n",
        "\n",
        "    # add padding for image to make it able to extract the number of patches equal to number of pixels\n",
        "    # the pad width should eqal (patch size - 1) / 2,  - center pixel will always lay in original image\n",
        "    nb_padding_pixels = int(size/2 - 0.5)\n",
        "    # choose symmetric mode\n",
        "    image_padded = pad(image, nb_padding_pixels, 'symmetric')\n",
        "\n",
        "    patch_size = (size, size, image.shape[2])\n",
        "    patches = view_as_windows(image_padded, patch_size)[:, :, 0]\n",
        "    return patches\n",
        "\n",
        "def grey_to_lbp(image):\n",
        "\n",
        "    image_shape = image.shape\n",
        "    image_width = image_shape[0]\n",
        "    image_height = image_shape[1]\n",
        "    image_depth = image_shape[2]\n",
        "\n",
        "    # print(\"Shape: {}, Width: {}, Height: {}, Depth: {}\".format(image_shape, image_width, image_height, image_depth))\n",
        "\n",
        "    #get the image shape\n",
        "    central_pixel = image[int((image_width - 1) / 2), int((image_height - 1) / 2), :]\n",
        "\n",
        "    # print(\"Central pixel: {}\".format(central_pixel))\n",
        "\n",
        "    lbp_map = np.zeros(image.shape)\n",
        "    # dimensions of the image: (image_width,image_height,image_depth)\n",
        "    # for each of k channels\n",
        "    for k in range(image_depth):\n",
        "        # compare each pixel greyscale value with the central pixel\n",
        "        for i in range(image_width):\n",
        "            for j in range(image_height):\n",
        "                if image[i,j,k] >= central_pixel[k]:\n",
        "                    lbp_map[i,j,k] = 1\n",
        "                elif image[i,j,k] < central_pixel[k]:\n",
        "                    lbp_map[i,j,k] = 0\n",
        "                # setting the central pixel to 0.5 removes the noisy pixel from the middle of a image\n",
        "                #but LBP map stops to be binary\n",
        "                #when using it, remove the eqaul sign from the first conditional equation\n",
        "                # else:\n",
        "                #     lbp_map[i,j,k] = 0.5\n",
        "\n",
        "    print('lbp map: {}'.format(lbp_map))\n",
        "\n",
        "    return lbp_map\n",
        "\n",
        "\n",
        "def grey_to_clbp(image):\n",
        "    #here, the treshold will be set as mean of the image, not as value of specific central pixel\n",
        "\n",
        "    image_shape = image.shape\n",
        "    image_width = image_shape[0]\n",
        "    image_height = image_shape[1]\n",
        "    image_depth = image_shape[2]\n",
        "\n",
        "    # print(\"Shape: {}, Width: {}, Height: {}, Depth: {}\".format(image_shape, image_width, image_height, image_depth))\n",
        "\n",
        "    #get the image shape\n",
        "    central_pixel = image[int((image_width - 1) / 2), int((image_height - 1) / 2), :]\n",
        "\n",
        "    # print(\"Central pixel: {}\".format(central_pixel))\n",
        "\n",
        "    clbp_map = np.zeros(image.shape)\n",
        "    # dimensions of the image: (image_width,image_height,image_depth)\n",
        "    # for each of k channels\n",
        "    for k in range(image_depth):\n",
        "        # compare each pixel greyscale value with the mean of the image\n",
        "\n",
        "        #get mean of a layer\n",
        "        channel_mean = np.mean(image[:, :, k])\n",
        "        for i in range(image_width):\n",
        "            for j in range(image_height):\n",
        "                if image[i,j,k] >= channel_mean:\n",
        "                    clbp_map[i,j,k] = 1\n",
        "                elif image[i,j,k] < channel_mean:\n",
        "                    clbp_map[i,j,k] = 0\n",
        "\n",
        "    return clbp_map\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def hs_to_grey(image):\n",
        "    return np.mean(image, axis = 2)\n",
        "\n",
        "def arr2D_to_list(arr: np.array):\n",
        "    dims = len(arr.shape)\n",
        "    lst = []\n",
        "    if dims == 2:\n",
        "        print('Gt width: {}, length: {}'.format(arr.shape[0], arr.shape[1]))\n",
        "        for i in range(arr.shape[0]):\n",
        "            for j in range(arr.shape[1]):\n",
        "                lst.append(arr[i, j])\n",
        "\n",
        "        return lst\n",
        "    else:\n",
        "        raise Exception('Array should be two dimentional')\n",
        "\n",
        "def arr5D_to_list_of_3D_arr(arr: np.array):\n",
        "    dims = len(arr.shape)\n",
        "    lst = []\n",
        "    if dims == 5:\n",
        "        for i in range(arr.shape[0]):\n",
        "            for j in range(arr.shape[1]):\n",
        "                    lst.append(arr[i, j])\n",
        "        return lst\n",
        "    else:\n",
        "        raise Exception('Array should be two dimentional')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcYarJ0qWToz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6f989c02-4c6a-4129-acb7-81cd00541109"
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9aqlGP74MAJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# model definition function\n",
        "def create_model(l2_loss_lambda = None, original_dim = (3, 3, 103)):\n",
        "    keras.backend.clear_session()\n",
        "\n",
        "    target_size = (32, 32)\n",
        "    l2 = None if l2_loss_lambda is None else keras.regularizers.l2(l2_loss_lambda)\n",
        "    if l2 is not None:\n",
        "        print('Using L2 regularization - l2_loss_lambda = %.4f' % l2_loss_lambda)\n",
        "\n",
        "    model = keras.Sequential(\n",
        "        [\n",
        "            keras.layers.Lambda(lambda image: tf.image.resize(image, target_size)),\n",
        "            keras.layers.Conv2D(256, 3, activation=tf.nn.relu, input_shape=(32, 32, 103)),\n",
        "            keras.layers.MaxPool2D(2),\n",
        "            keras.layers.Conv2D(512, 3, activation=tf.nn.relu),\n",
        "            keras.layers.MaxPool2D(2),\n",
        "            keras.layers.Flatten(),\n",
        "            keras.layers.Dense(512, activation=tf.nn.relu, kernel_regularizer=l2),\n",
        "            keras.layers.Dropout(0.5),\n",
        "            keras.layers.Dense(10, activation=tf.nn.softmax),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # model compiling\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\", f1_m]\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9yw3o594TxW",
        "colab_type": "code",
        "outputId": "f8d759ee-bd84-4909-92ce-4d99aad88a8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from skimage.transform import resize\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "mat = get_data(\"/content/drive/My Drive/PaviaU.mat\")\n",
        "# print(mat)\n",
        "\n",
        "mat_gt = get_data(\"/content/drive/My Drive/PaviaU_gt.mat\")\n",
        "# print(mat_gt\n",
        "\n",
        "data = mat['paviaU']\n",
        "ground_truth = mat_gt['paviaU_gt']\n",
        "\n",
        "print(\"Shape of the cube: {}\".format(data.shape))\n",
        "print(\"Shape of the labels_arr: {}\".format(ground_truth.shape))\n",
        "print(\"Number of classes: {}\".format(np.unique(ground_truth)))\n",
        "\n",
        "print(\"Maximum value: {}\".format(np.argmax(data)))\n",
        "\n",
        "print(\"Index of maximum value: {}\".format(np.unravel_index(np.argmax(data), data.shape)))\n",
        "\n",
        "\n",
        "# print(get_data._hs_to_grey(data_patches_hsi[8,5,0]))\n",
        "\n",
        "# apply lbp and clbp on image\n",
        "lbp_image = grey_to_lbp(data)\n",
        "print('LBP image shape: {}'.format(lbp_image.shape))\n",
        "# clbp_image = hsi_preprocessing.grey_to_clbp(data)\n",
        "\n",
        "#crop patches\n",
        "data_patches_lbp = get_patches(lbp_image, 3)\n",
        "print('LBP patches shape: {}'.format(data_patches_lbp.shape))\n",
        "# data_patches_clbp = hsi_preprocessing.get_patches(clbp_image, 3)\n",
        "\n",
        "'''\n",
        "Plotting images\n",
        "'''\n",
        "\n",
        "'''\n",
        "#compare greyscale image with lbp map\n",
        "plt.figure(1)\n",
        "plt.imshow(hsi_preprocessing.hs_to_grey(data), cmap='gray')\n",
        "plt.title(\"Grayscale image\")\n",
        "\n",
        "plt.figure(2)\n",
        "plt.imshow(hsi_preprocessing.hs_to_grey(lbp_image), cmap='gray')\n",
        "plt.title(\"LBP image\")\n",
        "\n",
        "plt.figure(3)\n",
        "plt.imshow(hsi_preprocessing.hs_to_grey(clbp_image), cmap='gray')\n",
        "plt.title(\"CLBP image\")\n",
        "\n",
        "# present some patches of lbp data\n",
        "plt.figure(4)\n",
        "co_x = 0\n",
        "co_y = 0\n",
        "plt.imshow(hsi_preprocessing.hs_to_grey(data_patches_lbp[co_x, co_y, 0]), cmap='gray')\n",
        "plt.title(\"LBP sample patch of coordinates {}, {}\".format(co_x, co_y))\n",
        "\n",
        "# present some patches of lbp data\n",
        "plt.figure(5)\n",
        "co_x = 11\n",
        "co_y = 107\n",
        "plt.imshow(hsi_preprocessing.hs_to_grey(data_patches_clbp[co_x, co_y, 0]), cmap='gray')\n",
        "plt.title(\"LBP sample patch of coordinates {}, {}\".format(co_x, co_y))\n",
        "\n",
        "plt.figure(6)\n",
        "plt.imshow(ground_truth)\n",
        "plt.title('Classes')\n",
        "\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Save patches and ground truth into arrays\n",
        "'''\n",
        "# patches\n",
        "patch_lbp_arr = np.asarray(arr5D_to_list_of_3D_arr(data_patches_lbp))\n",
        "print('patch_lbp_list len: {}'.format(len(patch_lbp_arr)))\n",
        "\n",
        "# ground truth\n",
        "labels_gt = np.asarray(arr2D_to_list(ground_truth))\n",
        "print(\"Labels len: {}\".format(len(labels_gt)))\n",
        "print(np.unique(labels_gt))\n",
        "\n",
        "\n",
        "\n",
        "lbp_image = None\n",
        "data_patches_lbp = None\n",
        "\n",
        "'''\n",
        "ImageDataGenerator and it's instances\n",
        "'''\n",
        "\n",
        "\n",
        "val_split = 0.25\n",
        "X_train, X_val, y_train, y_val = train_test_split(patch_lbp_arr, labels_gt, test_size=val_split, stratify=labels_gt)\n",
        "\n",
        "patch_lbp_arr = None\n",
        "label_gt = None\n",
        "# patch_lbp_arr = np.concatenate((X_train, X_val))\n",
        "# labels_gt = np.concatenate((y_train, y_val))\n",
        "\n",
        "# image_gen = ImageDataGenerator(rotation_range=20,\n",
        "#                                width_shift_range=0.1,\n",
        "#                                height_shift_range=0.1,\n",
        "#                                shear_range=0.1,\n",
        "#                                zoom_range=0.1,\n",
        "#                                horizontal_flip=True,\n",
        "#                                validation_split=0.5,\n",
        "#                                fill_mode='nearest')\n",
        "#\n",
        "# train_generator = image_gen.flow(\n",
        "#     patch_lbp_arr,\n",
        "#     labels_gt,\n",
        "#     shuffle=True,\n",
        "#     batch_size=256,\n",
        "#     subset='training')  # set as training data\n",
        "#\n",
        "# # data image generator is able to automaticaly split data for training and validation\n",
        "# validation_generator = image_gen.flow(\n",
        "#     patch_lbp_arr,\n",
        "#     labels_gt,\n",
        "#     shuffle=True,\n",
        "#     batch_size=256,\n",
        "#     subset='validation')  # set as validation data\n",
        "\n",
        "\n",
        "'''\n",
        "Use image data generator for further preprocessing, split for training and validation\n",
        "'''\n",
        "model = create_model(None, (5, 5, 103))\n",
        "\n",
        "# print(model.summary())\n",
        "\n",
        "history = model.fit(X_train, y_train, batch_size = 256, epochs=20, validation_data=(X_val, y_val))\n",
        "\n",
        "plt.figure(1)\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.legend(['train accuracy', 'val accuracy'])\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Create model\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Train and evaluate with apropriate metrics\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of the cube: (610, 340, 103)\n",
            "Shape of the labels_arr: (610, 340)\n",
            "Number of classes: [0 1 2 3 4 5 6 7 8 9]\n",
            "Maximum value: 396299\n",
            "Index of maximum value: (11, 107, 58)\n",
            "lbp map: [[[0. 0. 0. ... 1. 1. 1.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 1. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 1. 1. 1.]\n",
            "  [0. 0. 0. ... 1. 1. 1.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 0. ... 1. 0. 0.]\n",
            "  [1. 1. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 1. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 1. 0. ... 1. 1. 1.]\n",
            "  [0. 0. 0. ... 1. 1. 1.]\n",
            "  [1. 0. 0. ... 1. 1. 1.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 1. 1. ... 0. 0. 0.]\n",
            "  [1. 1. 1. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 1. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 1. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1. ... 0. 0. 0.]\n",
            "  [1. 1. 1. ... 0. 0. 0.]\n",
            "  [1. 1. 1. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 1. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1. ... 0. 0. 0.]\n",
            "  [1. 1. 1. ... 0. 0. 0.]\n",
            "  [1. 1. 1. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 1. 0. ... 0. 0. 0.]\n",
            "  [1. 1. 0. ... 0. 0. 0.]]]\n",
            "LBP image shape: (610, 340, 103)\n",
            "LBP patches shape: (610, 340, 3, 3, 103)\n",
            "patch_lbp_list len: 207400\n",
            "Gt width: 610, length: 340\n",
            "Labels len: 207400\n",
            "[0 1 2 3 4 5 6 7 8 9]\n",
            "Epoch 1/20\n",
            "608/608 [==============================] - 119s 196ms/step - loss: 0.6487 - accuracy: 0.7953 - f1_m: 0.5912 - val_loss: 0.5439 - val_accuracy: 0.8075 - val_f1_m: 0.5927\n",
            "Epoch 2/20\n",
            "608/608 [==============================] - 118s 193ms/step - loss: 0.5434 - accuracy: 0.8064 - f1_m: 0.5873 - val_loss: 0.5216 - val_accuracy: 0.8087 - val_f1_m: 0.6045\n",
            "Epoch 3/20\n",
            "608/608 [==============================] - 117s 193ms/step - loss: 0.5153 - accuracy: 0.8118 - f1_m: 0.5930 - val_loss: 0.4909 - val_accuracy: 0.8191 - val_f1_m: 0.5561\n",
            "Epoch 4/20\n",
            "608/608 [==============================] - 117s 193ms/step - loss: 0.4939 - accuracy: 0.8178 - f1_m: 0.5934 - val_loss: 0.4712 - val_accuracy: 0.8241 - val_f1_m: 0.5736\n",
            "Epoch 5/20\n",
            "608/608 [==============================] - 117s 193ms/step - loss: 0.4713 - accuracy: 0.8236 - f1_m: 0.5905 - val_loss: 0.4660 - val_accuracy: 0.8257 - val_f1_m: 0.5714\n",
            "Epoch 6/20\n",
            "608/608 [==============================] - 117s 193ms/step - loss: 0.4573 - accuracy: 0.8282 - f1_m: 0.5885 - val_loss: 0.4609 - val_accuracy: 0.8292 - val_f1_m: 0.6007\n",
            "Epoch 7/20\n",
            "608/608 [==============================] - 117s 193ms/step - loss: 0.4400 - accuracy: 0.8338 - f1_m: 0.5816 - val_loss: 0.4492 - val_accuracy: 0.8334 - val_f1_m: 0.5919\n",
            "Epoch 8/20\n",
            "608/608 [==============================] - 117s 193ms/step - loss: 0.4285 - accuracy: 0.8377 - f1_m: 0.5792 - val_loss: 0.4542 - val_accuracy: 0.8331 - val_f1_m: 0.5739\n",
            "Epoch 9/20\n",
            "608/608 [==============================] - 117s 193ms/step - loss: 0.4148 - accuracy: 0.8420 - f1_m: 0.5739 - val_loss: 0.4388 - val_accuracy: 0.8370 - val_f1_m: 0.5718\n",
            "Epoch 10/20\n",
            "608/608 [==============================] - 117s 193ms/step - loss: 0.4017 - accuracy: 0.8459 - f1_m: 0.5733 - val_loss: 0.4433 - val_accuracy: 0.8373 - val_f1_m: 0.5639\n",
            "Epoch 11/20\n",
            "608/608 [==============================] - 117s 193ms/step - loss: 0.3878 - accuracy: 0.8509 - f1_m: 0.5678 - val_loss: 0.4372 - val_accuracy: 0.8403 - val_f1_m: 0.5553\n",
            "Epoch 12/20\n",
            "608/608 [==============================] - 117s 193ms/step - loss: 0.3778 - accuracy: 0.8539 - f1_m: 0.5648 - val_loss: 0.4547 - val_accuracy: 0.8406 - val_f1_m: 0.5435\n",
            "Epoch 13/20\n",
            "608/608 [==============================] - 118s 193ms/step - loss: 0.3668 - accuracy: 0.8574 - f1_m: 0.5600 - val_loss: 0.4497 - val_accuracy: 0.8397 - val_f1_m: 0.5549\n",
            "Epoch 14/20\n",
            "608/608 [==============================] - 118s 195ms/step - loss: 0.3561 - accuracy: 0.8618 - f1_m: 0.5588 - val_loss: 0.4578 - val_accuracy: 0.8372 - val_f1_m: 0.5638\n",
            "Epoch 15/20\n",
            "608/608 [==============================] - 118s 195ms/step - loss: 0.3468 - accuracy: 0.8650 - f1_m: 0.5542 - val_loss: 0.4551 - val_accuracy: 0.8413 - val_f1_m: 0.5496\n",
            "Epoch 16/20\n",
            "608/608 [==============================] - 118s 195ms/step - loss: 0.3424 - accuracy: 0.8662 - f1_m: 0.5529 - val_loss: 0.4605 - val_accuracy: 0.8430 - val_f1_m: 0.5437\n",
            "Epoch 17/20\n",
            "608/608 [==============================] - 119s 195ms/step - loss: 0.3285 - accuracy: 0.8715 - f1_m: 0.5497 - val_loss: 0.4696 - val_accuracy: 0.8451 - val_f1_m: 0.5328\n",
            "Epoch 18/20\n",
            "608/608 [==============================] - 118s 195ms/step - loss: 0.3243 - accuracy: 0.8720 - f1_m: 0.5483 - val_loss: 0.4763 - val_accuracy: 0.8399 - val_f1_m: 0.5481\n",
            "Epoch 19/20\n",
            "608/608 [==============================] - 118s 194ms/step - loss: 0.3211 - accuracy: 0.8747 - f1_m: 0.5449 - val_loss: 0.5065 - val_accuracy: 0.8380 - val_f1_m: 0.5456\n",
            "Epoch 20/20\n",
            "608/608 [==============================] - 118s 194ms/step - loss: 0.3106 - accuracy: 0.8776 - f1_m: 0.5434 - val_loss: 0.5130 - val_accuracy: 0.8434 - val_f1_m: 0.5285\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nTrain and evaluate with apropriate metrics\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7FBRIr29Bv-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "cae7c4a9-e5ad-4a8b-9f99-68664cdbfb76"
      },
      "source": [
        "predictions = model.predict(X_val)\n",
        "gt_predicted = [np.argmax(prediction) for prediction in predictions]\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "51850\n",
            "51850\n",
            "[0 1 2 3 4 5 6 7 8 9]\n",
            "[0 1 2 3 4 5 6 7 8 9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5Jng0lEjRKb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "792a2827-d158-4f87-9b75-9afa36a2d3c9"
      },
      "source": [
        "print(len(y_val))\n",
        "print(len(gt_predicted))\n",
        "print(np.unique(y_val))\n",
        "print(np.unique(gt_predicted))\n",
        "print(y_val[0:19])\n",
        "print(np.array(gt_predicted[0:19]))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "51850\n",
            "51850\n",
            "[0 1 2 3 4 5 6 7 8 9]\n",
            "[0 1 2 3 4 5 6 7 8 9]\n",
            "[0 0 1 0 0 2 0 0 2 0 0 0 0 0 6 0 2 0 0]\n",
            "[0 0 1 0 0 2 0 0 2 0 0 5 0 0 0 0 2 5 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64e_WE5a9OxN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "outputId": "c897f025-4ed6-4d5d-891e-859ceb4097d8"
      },
      "source": [
        "kappa = tfa.metrics.CohenKappa(num_classes=5)\n",
        "kappa.update_state(np.array([4, 4, 3, 4, 2, 4, 1, 1], dtype=np.int32)\n",
        ", np.array([4, 4, 3, 4, 4, 2, 1, 1], dtype=np.int32))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-47dd8a4951d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mkappa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCohenKappa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m kappa.update_state(np.array([4, 4, 3, 4, 2, 4, 1, 1], dtype=np.int32)\n\u001b[0;32m----> 3\u001b[0;31m , np.array([4, 4, 3, 4, 4, 2, 1, 1], dtype=np.int32))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/metrics_utils.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(metric_obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_context_for_symbolic_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m       \u001b[0mupdate_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_state_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mupdate_op\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# update_op will be None in eager execution.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m       \u001b[0mmetric_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_addons/metrics/cohens_kappa.py\u001b[0m in \u001b[0;36mupdate_state\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m    133\u001b[0m           \u001b[0mUpdate\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \"\"\"\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_binary_class_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_addons/metrics/cohens_kappa.py\u001b[0m in \u001b[0;36m_update_multi_class_model\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_addons/metrics/cohens_kappa.py\u001b[0m in \u001b[0;36m_update_confusion_matrix\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             dtype=tf.float32)\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf_mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_conf_mtx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/confusion_matrix.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(labels, predictions, num_classes, weights, dtype, name)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m     \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m     values = (array_ops.ones_like(predictions, dtype)\n\u001b[1;32m    194\u001b[0m               if weights is None else weights)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1338\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mexpanded_num_dims\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mexpanded_num_dims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m       raise ValueError(\"axis = %d not in [%d, %d)\" %\n\u001b[0;32m-> 1340\u001b[0;31m                        (axis, -expanded_num_dims, expanded_num_dims))\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: axis = 1 not in [-1, 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLkXzHZBn0E3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "db8b28b8-7179-4392-be37-5dce495cf6bc"
      },
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "kappa = cohen_kappa_score(y_val, np.array(gt_predicted))\n",
        "print(kappa)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4434326069208392\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}